---
layout: post
title: LASSO v.s. Ridge Regression -- L1 v.s. L2 norm
---
<!--
如何在github上的md file放入數學式：https://www.youtube.com/watch?v=dpVnmxpVdvg
在latex線上編輯器(http://latex.codecogs.com/eqneditor/editor.php)中輸入數學式，複製圖片網址，然後貼到以下![name](address)即可顯示數學式圖片
例如：![l2 nrom](https://latex.codecogs.com/svg.latex?\Large&space;\left\|\beta\right\|^{2}_{2})
或直接輸入latex數學代碼於 “ https://latex.codecogs.com/svg.latex?\Large&space; ” 後
例如：<img src="https://latex.codecogs.com/svg.latex?\Large&space;x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}" title="\Large x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}" />
<!--
如何在github上的md/html file放入特殊符號如希臘字母
https://brajeshwar.github.io/entities/
-->
## 什麼是LASSO/Ridge Regression？
把原本要最小化的目標函數 (loss/cost function) 再加上一個對解空間的限制條件。LASSO是限制解的L1 norm，Ridge是限制解的L2 norm。
<!-- more -->  
如式子

![lasso](https://latex.codecogs.com/svg.latex?\Large&space;\Sigma_{i=1}^{n}{(y_{i}-x_{i}\beta)^{2}}+{\lambda\|\beta\|_{1}})  是 LASSO 的目標函數

![ridge](https://latex.codecogs.com/svg.latex?\Large&space;\Sigma_{i=1}^{n}{(y_{i}-x_{i}\beta)^{2}}+\lambda\left\|\beta\right\|^{2}_{2})   是 Ridge 的目標函數

![beta matrix](https://latex.codecogs.com/gif.latex?%5Cbeta%3D%20%5Cbegin%7Bbmatrix%7D%20%5Cbeta_0%5C%5C%20%5Cbeta_1%5C%5C%20%5Cvdots%5C%5C%20%5Cbeta_p%20%5Cend%7Bbmatrix%7D)  為我們有興趣/想找到的未知參數

![lambda](https://latex.codecogs.com/svg.latex?\Large&space;\lambda)  為限制解空間、手動調整的參數。是一種自己設定的超參數。


## 為什麼需要LASSO和Ridge Regression？
### 起源 

線性迴歸(Linear Regression)分析的目的，是找到一個符合特徵資料趨勢的線性模型。（注意：這裡的線性，是指對參數而言函數為線性，也就是係數的線性函數。）

線性迴歸最常使用最小平方法來找出最小化「誤差平方和」的參數以做為參數的解(least squares estimate)。最小平方法的解空間幾乎沒有限制，因此人們試圖給予整個解空間一些限制，才有了以 L<sup>1</sup>-norm 限制解空間的 LASSO Regression ，和 L<sup>2</sup>-norm 限制解空間的 Ridge Regression。這些方法又稱 L<sup>1</sup>-norm/L<sup>2</sup>-norm penalty/regularization。


#### 什麼是Norm?

Mathematically a norm is a total size or length of all vectors in a vector space  or matrices.

定義上，令一個向量

![vector x](https://latex.codecogs.com/gif.latex?x%3D%20%5Cbegin%7Bbmatrix%7D%20x_1%5C%5C%20x_2%5C%5C%20%5Cvdots%20%5C%5C%20x_n%20%5Cend%7Bbmatrix%7D)

則

L<sup>1</sup>-norm 為 ![l1 nrom](https://latex.codecogs.com/svg.latex?\Large&space;\|x\|_{1}=\Sigma_{i=1}^{n}{&#124;x_{i}&#124;})

L<sup>2</sup>-norm 為 ![l2 nrom](https://latex.codecogs.com/svg.latex?\Large&space;\|x\|_{2}=\sqrt{\Sigma_{i=1}^{n}{x_{i}^{2}}})


## 技術

假設有 n 筆資料，每筆資料有p個特徵。假設特徵資料(x)和目標變數(y)有線性關係。則我們可令

![linear model](https://latex.codecogs.com/svg.latex?\Large&space;y=\beta_{0}+\beta_{1}x_{1}+...+\beta_{p}x_{p}+\epsilon=x\beta+\epsilon)

![x vector](https://latex.codecogs.com/svg.latex?\Large&space;x=\[1,x_{1},...,x_{p}\]) 為特徵資料矩陣

![beta matrix](https://latex.codecogs.com/gif.latex?%5Cbeta%3D%20%5Cbegin%7Bbmatrix%7D%20%5Cbeta_0%5C%5C%20%5Cbeta_1%5C%5C%20%5Cvdots%5C%5C%20%5Cbeta_p%20%5Cend%7Bbmatrix%7D) 為未知參數

![epsilon](https://latex.codecogs.com/svg.latex?\Large&space;\epsilon) 為誤差

則最小平方法是找出最小化 ![least square](https://latex.codecogs.com/svg.latex?\Large&space;\Sigma_{i=1}^{n}{(y_{i}-x_{i}\beta)^{2}}) 的 ![beta](https://latex.codecogs.com/svg.latex?\Large&space;\beta)

#### LASSO -- L<sup>1</sup>-norm Penalty/Regularization

LASSO Regression 的目標是找出最小化 ![lasso](https://latex.codecogs.com/svg.latex?\Large&space;\Sigma_{i=1}^{n}{(y_{i}-x_{i}\beta)^{2}}+{\lambda\|\beta\|_{1}}) 的 ![beta](https://latex.codecogs.com/svg.latex?\Large&space;\beta)

![lambda](https://latex.codecogs.com/svg.latex?\Large&space;\lambda) 為限制解空間、手動調整的參數。

參考 [LASSO - Wikipedia](https://en.wikipedia.org/wiki/Lasso_(statistics))

#### L<sup>2</sup>-norm Penalty/Regularization

LASSO Regression 的目標是找出最小化 ![ridge](https://latex.codecogs.com/svg.latex?\Large&space;\Sigma_{i=1}^{n}{(y_{i}-x_{i}\beta)^{2}}+\lambda\left\|\beta\right\|^{2}_{2}) 的 ![beta](https://latex.codecogs.com/svg.latex?\Large&space;\beta)

同樣地， &lambda;為限制解空間、手動調整的參數。


本文想針對LASSO做更多的探討。



## 特色

|        |   優點  |  缺點   |
| ------ | ------- | ------ |
|L1 norm |  sparse |        |
|L2 norm |         |        |



## 應用

### feature selection

### 解決/避免 overfitting
